{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luthfan05/Interpolasi-Lokasi-Hujan/blob/main/01_CH_Bulanan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "v0sm4tO-FcM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rasterio cfgrib pyhdf netCDF4 gdal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YOXlKUQNbms",
        "outputId": "a79ca32a-f377-48df-dab6-2e42e2582263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting cfgrib\n",
            "  Downloading cfgrib-0.9.15.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyhdf\n",
            "  Downloading pyhdf-0.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: gdal in /usr/local/lib/python3.11/dist-packages (3.6.4)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Collecting eccodes>=0.9.8 (from cfgrib)\n",
            "  Downloading eccodes-2.41.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (14 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from eccodes>=0.9.8->cfgrib) (1.17.1)\n",
            "Collecting findlibs (from eccodes>=0.9.8->cfgrib)\n",
            "  Downloading findlibs-0.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->eccodes>=0.9.8->cfgrib) (2.22)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgrib-0.9.15.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyhdf-0.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.3/780.3 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading eccodes-2.41.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading findlibs-0.1.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: findlibs, pyhdf, cligj, click-plugins, cftime, affine, rasterio, netCDF4, eccodes, cfgrib\n",
            "Successfully installed affine-2.4.0 cfgrib-0.9.15.0 cftime-1.6.4.post1 click-plugins-1.1.1 cligj-0.7.2 eccodes-2.41.0 findlibs-0.1.1 netCDF4-1.7.2 pyhdf-0.11.6 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGHZPeOY1rNB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import h5py\n",
        "import re\n",
        "from datetime import datetime\n",
        "import calendar\n",
        "from glob import glob\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "from scipy.interpolate import RegularGridInterpolator, LinearNDInterpolator\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import rasterio\n",
        "from netCDF4 import Dataset, num2date\n",
        "from pyhdf.SD import SD, SDC\n",
        "from osgeo import gdal\n",
        "import cfgrib\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Source"
      ],
      "metadata": {
        "id": "fw6__pUTFgv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4_z_1XB-gc"
      },
      "source": [
        "## CHIRPS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔹 Konfigurasi folder\n",
        "source_folder = '/content/drive/MyDrive/01-CURAH_HUJAN/02-CHIRPS/Unduhan'\n",
        "extract_folder = '/content/ekstrak'\n",
        "output_folder = '/content/final_output'\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# 🔹 Batas koordinat wilayah Indonesia\n",
        "lat_min, lat_max = -12.0, -5.5\n",
        "lon_min, lon_max = 105.0, 115.0\n",
        "\n",
        "tar_files = [f for f in os.listdir(source_folder) if f.endswith(\".tar.gz\")]\n",
        "\n",
        "def extract_date_flexible(filename):\n",
        "    # Modifikasi sesuai pola nama file kamu\n",
        "    import re\n",
        "    match = re.search(r\"\\d{4}[\\._-]?\\d{2}\", filename)\n",
        "    return match.group(0).replace(\"_\", \"-\").replace(\".\", \"-\") if match else \"unknown\"\n",
        "\n",
        "for tar_file in tar_files:\n",
        "    try:\n",
        "        tar_path = os.path.join(source_folder, tar_file)\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            members = tar.getmembers()\n",
        "            for m in members:\n",
        "\n",
        "            # Ekstrak semua file .bil dan file pendukung lainnya\n",
        "            relevant_exts = [\".bil\", \".hdr\", \".blw\", \".prj\"]\n",
        "            extracted_names = []\n",
        "\n",
        "            for member in members:\n",
        "                if any(member.name.endswith(ext) for ext in relevant_exts):\n",
        "                    member.name = os.path.basename(member.name)  # Hilangkan path folder\n",
        "                    tar.extract(member, path=extract_folder)\n",
        "                    extracted_names.append(member.name)\n",
        "\n",
        "\n",
        "            # Proses semua file .bil yang telah diekstrak\n",
        "            for filename in extracted_names:\n",
        "                if not filename.endswith(\".bil\"):\n",
        "                    continue\n",
        "\n",
        "                bil_file_path = os.path.join(extract_folder, filename)\n",
        "\n",
        "                with rasterio.open(bil_file_path) as dataset:\n",
        "                    data = dataset.read(1)\n",
        "                    data[data == dataset.nodata] = -10\n",
        "\n",
        "                    transform = dataset.transform\n",
        "                    nrows, ncols = dataset.height, dataset.width\n",
        "\n",
        "                    lons = np.array([transform[2] + i * transform[0] for i in range(ncols)])\n",
        "                    lats = np.array([transform[5] + j * transform[4] for j in range(nrows)])\n",
        "\n",
        "                    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
        "\n",
        "                    df = pd.DataFrame({\n",
        "                        \"lon\": lon_grid.flatten(),\n",
        "                        \"lat\": lat_grid.flatten(),\n",
        "                        \"ch\": data.flatten()\n",
        "                    })\n",
        "\n",
        "                    df = df[\n",
        "                        (df[\"lat\"].between(lat_min, lat_max)) &\n",
        "                        (df[\"lon\"].between(lon_min, lon_max))\n",
        "                    ].copy()\n",
        "\n",
        "                    df[\"source_file\"] = filename\n",
        "                    df[\"date\"] = extract_date_flexible(filename)\n",
        "\n",
        "                    # Simpan hasil ke CSV\n",
        "                    output_name = os.path.splitext(filename)[0] + \"_final.csv\"\n",
        "                    output_path = os.path.join(output_folder, output_name)\n",
        "                    df.to_csv(output_path, index=False)\n",
        "\n",
        "                # Hapus file sementara setelah diproses\n",
        "                os.remove(bil_file_path)\n",
        "\n",
        "                # Juga hapus file .hdr jika ada\n",
        "                hdr_path = bil_file_path.replace(\".bil\", \".hdr\")\n",
        "                if os.path.exists(hdr_path):\n",
        "                    os.remove(hdr_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gagal memproses {tar_file}: {e}\")\n",
        "\n",
        "print(\"🎉 Semua file selesai diproses.\")\n"
      ],
      "metadata": {
        "id": "xi24fjs5MnFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excel_file = '/content/drive/MyDrive/Shared/Koordinat CH.xlsx'\n",
        "df_points = pd.read_excel(excel_file)\n",
        "pos = df_points[\"Nama Pos\"].values\n",
        "lat_points = df_points[\"Y\"].values\n",
        "lon_points = df_points[\"X\"].values\n",
        "point_coords = np.array([lon_points, lat_points]).T\n",
        "\n",
        "def process_csv_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Ambil lon, lat, ch\n",
        "    lons = np.sort(df[\"lon\"].unique())\n",
        "    lats = np.sort(df[\"lat\"].unique())\n",
        "\n",
        "    # Bentuk grid dari data (diasumsikan grid lengkap)\n",
        "    grid_data = df.pivot_table(index=\"lat\", columns=\"lon\", values=\"ch\").values\n",
        "    # Hati-hati: pastikan urutan lat dari besar ke kecil sesuai orientasi data\n",
        "    if lats[0] > lats[-1]:\n",
        "        lats = lats[::-1]\n",
        "        grid_data = grid_data[::-1, :]\n",
        "\n",
        "    interpolator = RegularGridInterpolator((lats, lons), grid_data, bounds_error=False, fill_value=np.nan)\n",
        "\n",
        "    interpolated = interpolator(point_coords[:, [1, 0]])  # karena (lat, lon)\n",
        "\n",
        "    # Fallback ke nearest neighbor kalau ada NaN\n",
        "    if np.isnan(interpolated).any():\n",
        "        valid_mask = ~np.isnan(df[\"ch\"])\n",
        "        valid_points = df.loc[valid_mask, [\"lon\", \"lat\"]].values\n",
        "        valid_values = df.loc[valid_mask, \"ch\"].values\n",
        "\n",
        "        nan_mask = np.isnan(interpolated)\n",
        "        nearest_idx = np.argmin(cdist(point_coords[nan_mask], valid_points), axis=1)\n",
        "        interpolated[nan_mask] = valid_values[nearest_idx]\n",
        "\n",
        "    # Ambil tanggal dari nama file\n",
        "    filename = os.path.basename(file_path)\n",
        "    match = re.search(r'(\\d{6})_final\\.csv$', filename)\n",
        "    if match:\n",
        "        date = datetime.strptime(match.group(1), \"%Y%m\")\n",
        "    else:\n",
        "        date = None\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'pos': pos,\n",
        "        'lon': lon_points,\n",
        "        'lat': lat_points,\n",
        "        'precipitation': interpolated,\n",
        "        'date': date\n",
        "    })\n",
        "\n",
        "# Proses semua file CSV\n",
        "folder = \"/content/final_output/\"\n",
        "all_csv_files = sorted(glob(os.path.join(folder, '*_final.csv'), recursive=True))\n",
        "\n",
        "results = []\n",
        "\n",
        "for file_path in all_csv_files:\n",
        "    df_interp = process_csv_file(file_path)\n",
        "\n",
        "    # Simpan per file (opsional)\n",
        "    output_path = file_path.replace(\"_final.csv\", \"_interpolated.csv\")\n",
        "    df_interp.to_csv(output_path, index=False)\n",
        "\n",
        "    results.append(df_interp)\n",
        "\n",
        "print(\"✅ Semua file selesai diproses.\")\n",
        "\n",
        "# Gabung semua (opsional)\n",
        "df_all = pd.concat(results, ignore_index=True)\n",
        "df_all.to_csv('/content/drive/MyDrive /01-CURAH_HUJAN/Bulanan/CHIRPS 56.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnq5Mc_uL_sw",
        "outputId": "389dc922-4a01-45c9-85a9-7cd46f85eb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Semua file selesai diproses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔹 Konfigurasi folder\n",
        "source_folder = '/content/drive/MyDrive/01-CURAH_HUJAN/02-CHIRPS/Unduhan'\n",
        "extract_folder = '/content/ekstrak'\n",
        "output_folder = '/content/drive/MyDrive/01-CURAH_HUJAN/Grid/02-CHIRPS'\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# 🔹 Batas koordinat wilayah Indonesia\n",
        "lat_min, lat_max = -12.0, -5.5\n",
        "lon_min, lon_max = 105.0, 115.0\n",
        "\n",
        "tar_files = [f for f in os.listdir(source_folder) if f.endswith(\".tar.gz\")]\n",
        "\n",
        "def extract_date_flexible(filename):\n",
        "    # Modifikasi sesuai pola nama file kamu\n",
        "    import re\n",
        "    match = re.search(r\"\\d{4}[\\._-]?\\d{2}\", filename)\n",
        "    return match.group(0).replace(\"_\", \"-\").replace(\".\", \"-\") if match else \"unknown\"\n",
        "\n",
        "for tar_file in tar_files:\n",
        "    try:\n",
        "        tar_path = os.path.join(source_folder, tar_file)\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            members = tar.getmembers()\n",
        "            for m in members:\n",
        "              # Ekstrak semua file .bil dan file pendukung lainnya\n",
        "              relevant_exts = [\".bil\", \".hdr\", \".blw\", \".prj\"]\n",
        "              extracted_names = []\n",
        "\n",
        "            for member in members:\n",
        "                if any(member.name.endswith(ext) for ext in relevant_exts):\n",
        "                    member.name = os.path.basename(member.name)  # Hilangkan path folder\n",
        "                    tar.extract(member, path=extract_folder)\n",
        "                    extracted_names.append(member.name)\n",
        "\n",
        "\n",
        "            # Proses semua file .bil yang telah diekstrak\n",
        "            for filename in extracted_names:\n",
        "                if not filename.endswith(\".bil\"):\n",
        "                    continue\n",
        "\n",
        "                bil_file_path = os.path.join(extract_folder, filename)\n",
        "\n",
        "                with rasterio.open(bil_file_path) as dataset:\n",
        "                    data = dataset.read(1)\n",
        "                    data[data == dataset.nodata] = -10\n",
        "\n",
        "                    transform = dataset.transform\n",
        "                    nrows, ncols = dataset.height, dataset.width\n",
        "\n",
        "                    lons = np.array([transform[2] + i * transform[0] for i in range(ncols)])\n",
        "                    lats = np.array([transform[5] + j * transform[4] for j in range(nrows)])\n",
        "\n",
        "                    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
        "\n",
        "                    df = pd.DataFrame({\n",
        "                        \"lon\": lon_grid.flatten(),\n",
        "                        \"lat\": lat_grid.flatten(),\n",
        "                        \"ch\": data.flatten()\n",
        "                    })\n",
        "\n",
        "                    df[\"date\"] = extract_date_flexible(filename)\n",
        "                    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m\", errors=\"coerce\")\n",
        "\n",
        "                    # Simpan hasil ke CSV\n",
        "                    output_name = os.path.splitext(filename)[0] + \"_grid.csv\"\n",
        "                    output_path = os.path.join(output_folder, output_name)\n",
        "                    df.to_csv(output_path, index=False)\n",
        "\n",
        "                # Hapus file sementara setelah diproses\n",
        "                os.remove(bil_file_path)\n",
        "\n",
        "                # Juga hapus file .hdr jika ada\n",
        "                hdr_path = bil_file_path.replace(\".bil\", \".hdr\")\n",
        "                if os.path.exists(hdr_path):\n",
        "                    os.remove(hdr_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gagal memproses {tar_file}: {e}\")\n",
        "\n",
        "print(\"🎉 Semua file selesai diproses.\")\n"
      ],
      "metadata": {
        "id": "_W3B81L2G0Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLvrR5rfREr9"
      },
      "source": [
        "## IMERG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/GPM_3IMERGM_1998/3B-MO.MS.MRG.3IMERG.19980101-S000000-E235959.01.V07B.HDF5'\n",
        "\n",
        "excel_file = '/content/drive/MyDrive/Shared/Koordinat CH.xlsx'\n",
        "df_points = pd.read_excel(excel_file)\n",
        "pos = df_points[\"Nama Pos\"].values\n",
        "lat_points = df_points[\"Y\"].values  # Latitude\n",
        "lon_points = df_points[\"X\"].values  # Longitude\n",
        "point_coords = np.array([lon_points, lat_points]).T  # bentuk (N, 2)\n",
        "\n",
        "\n",
        "def process_imerg_file(file_path):\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        precip = f[\"Grid\"][\"precipitation\"][0, :, :]  # (lon, lat)\n",
        "        lat = f[\"Grid\"][\"lat\"][:]\n",
        "        lon = f[\"Grid\"][\"lon\"][:]\n",
        "\n",
        "    # Buat interpolator: RegularGridInterpolator expects (x, y) = (lon, lat)\n",
        "    interpolator = RegularGridInterpolator((lon, lat), precip, bounds_error=False, fill_value=np.nan)\n",
        "\n",
        "    # Interpolasi bilinear\n",
        "    interpolated = interpolator(point_coords)\n",
        "\n",
        "    # Nearest neighbor untuk NaN\n",
        "    if np.isnan(interpolated).any():\n",
        "        valid_mask = ~np.isnan(precip)\n",
        "        lon_grid, lat_grid = np.meshgrid(lon, lat, indexing='ij')\n",
        "        valid_points = np.array([lon_grid[valid_mask], lat_grid[valid_mask]]).T\n",
        "        valid_values = precip[valid_mask]\n",
        "\n",
        "        nan_mask = np.isnan(interpolated)\n",
        "        nearest_idx = np.argmin(cdist(point_coords[nan_mask], valid_points), axis=1)\n",
        "        interpolated[nan_mask] = valid_values[nearest_idx]\n",
        "\n",
        "    # Ambil tanggal dari nama file\n",
        "    filename = os.path.basename(file_path)\n",
        "    try:\n",
        "        date_str = filename.split('.')[4].split('-')[0]  # Contoh: '19980101'\n",
        "        date = datetime.strptime(date_str, \"%Y%m%d\")\n",
        "    except Exception:\n",
        "        date = None\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'pos': pos,\n",
        "        'lon': lon_points,\n",
        "        'lat': lat_points,\n",
        "        'precipitation': interpolated,\n",
        "        'date': date\n",
        "    })\n",
        "\n",
        "folder = \"/content/drive/MyDrive/03-IMERG_GPM/\"\n",
        "all_files = sorted(glob(os.path.join(folder,'**', \"*.HDF5\")))\n",
        "\n",
        "results = []\n",
        "\n",
        "for file_path in all_files:\n",
        "    df_interp = process_imerg_file(file_path)\n",
        "\n",
        "    # Simpan per file (opsional)\n",
        "    output_path = file_path.replace(\".HDF5\", \"_interpolated.csv\")\n",
        "    df_interp.to_csv(output_path, index=False)\n",
        "\n",
        "    # Atau gabungkan semua ke satu list\n",
        "    results.append(df_interp)\n",
        "print(\"Selesai\")\n",
        "\n",
        "# (Opsional) Gabung semua hasil\n",
        "df_all = pd.concat(results, ignore_index=True)\n",
        "df_all.to_csv(\"/content/drive/MyDrive/Bulanan/interpolated_all.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "oRYQZuKq9zyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x17HNCNBy0s"
      },
      "source": [
        "## GSMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbwxZjSSGp7R",
        "outputId": "8e52edd8-1667-46a0-cb00-074f4e04f235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selesai\n"
          ]
        }
      ],
      "source": [
        "base_dir = '/content/drive/MyDrive/01-CURAH_HUJAN/04-GSMAP/'\n",
        "output_dir = '/content/drive/MyDrive/Shared/00 Hujan/04-GSMaP_csv'  # folder tujuan hasil\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Cari semua file .dat.gz\n",
        "file_list = glob(os.path.join(base_dir, '**', '*.dat.gz'), recursive=True)\n",
        "\n",
        "for file_path in file_list:\n",
        "    # Ekstrak tanggal dari nama file\n",
        "    basename = os.path.basename(file_path)\n",
        "    yyyymm = basename.split('.')[1]\n",
        "    date = pd.to_datetime(yyyymm, format=\"%Y%m\")\n",
        "\n",
        "    # Baca file binary\n",
        "    with gzip.open(file_path, \"rb\") as f:\n",
        "        data = np.frombuffer(f.read(), dtype='<f4')\n",
        "\n",
        "    rain_rate = data[:4320000].copy()\n",
        "    valid_pixel_count = data[4320000:]\n",
        "\n",
        "    # Handle missing value\n",
        "    rain_rate[rain_rate == -999.9] = np.nan\n",
        "\n",
        "    # Buat grid\n",
        "    lat = np.linspace(59.95, -59.95, 1200)\n",
        "    lon = np.linspace(-179.95, 179.95, 3600)\n",
        "    lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
        "\n",
        "    # DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        \"lon\": lon_grid.flatten(),\n",
        "        \"lat\": lat_grid.flatten(),\n",
        "        \"rain_rate\": rain_rate,\n",
        "        \"valid_pixel_count\": valid_pixel_count\n",
        "    })\n",
        "    df[\"total_rain_month\"] = df[\"rain_rate\"] * df[\"valid_pixel_count\"]\n",
        "    df[\"date\"] = date\n",
        "\n",
        "    # 🎯 Filter hanya Indonesia\n",
        "    df = df[(df[\"lat\"] >= -11) & (df[\"lat\"] <= 6) &\n",
        "            (df[\"lon\"] >= 95) & (df[\"lon\"] <= 141)]\n",
        "\n",
        "    # Simpan per file\n",
        "    output_file = os.path.join(output_dir, f\"gsmap_{yyyymm}.csv\")\n",
        "    df.to_csv(output_file, index=False)\n",
        "print(\"Selesai\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load titik-titik target\n",
        "excel_file = '/content/drive/MyDrive/Shared/Koordinat CH.xlsx'\n",
        "df_points = pd.read_excel(excel_file)\n",
        "pos = df_points[\"Nama Pos\"].values\n",
        "lat_points = df_points[\"Y\"].values\n",
        "lon_points = df_points[\"X\"].values\n",
        "points = np.array([lat_points, lon_points]).T\n",
        "\n",
        "# 2. Siapkan folder CSV GSMaP\n",
        "csv_dir = '/content/drive/MyDrive/Shared/00 Hujan/04-GSMaP_csv'\n",
        "csv_files = sorted(glob(os.path.join(csv_dir, \"*.csv\")))\n",
        "\n",
        "# 3. Simpan hasil akhir\n",
        "interpolated_all = []\n",
        "\n",
        "# 4. Loop semua file GSMaP per bulan\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Ambil hanya Indonesia (opsional, karena sudah dipotong)\n",
        "    lat = np.sort(df['lat'].unique())[::-1]  # descending\n",
        "    lon = np.sort(df['lon'].unique())        # ascending\n",
        "\n",
        "    # Buat grid 2D untuk total_rain_month\n",
        "    grid_values = df.pivot(index='lat', columns='lon', values='total_rain_month').values\n",
        "\n",
        "    # Interpolasi Bilinear\n",
        "    interpolator = RegularGridInterpolator((lat, lon), grid_values, bounds_error=False, fill_value=np.nan)\n",
        "    interp_values = interpolator(points)\n",
        "\n",
        "    # Cek NaN, ganti dengan nearest neighbor\n",
        "    nan_mask = np.isnan(interp_values)\n",
        "    if nan_mask.any():\n",
        "        valid_mask = ~np.isnan(grid_values)\n",
        "        lon_grid, lat_grid = np.meshgrid(lon, lat)\n",
        "        valid_points = np.array([lat_grid[valid_mask], lon_grid[valid_mask]]).T\n",
        "        valid_values = grid_values[valid_mask]\n",
        "\n",
        "        if len(valid_points) > 0:\n",
        "            nearest_idx = np.argmin(cdist(points[nan_mask], valid_points), axis=1)\n",
        "            interp_values[nan_mask] = valid_values[nearest_idx]\n",
        "\n",
        "    # Simpan hasil bulan ini\n",
        "    yyyymm = os.path.basename(file).split(\"_\")[1].split(\".\")[0]\n",
        "    df_month = pd.DataFrame({\n",
        "        \"date\": pd.to_datetime(yyyymm, format=\"%Y%m\"),\n",
        "        'pos': pos,\n",
        "        \"lon\": lon_points,\n",
        "        \"lat\": lat_points,\n",
        "        \"rain\": interp_values\n",
        "    })\n",
        "    interpolated_all.append(df_month)\n",
        "\n",
        "# 5. Gabung semua bulan\n",
        "df_final = pd.concat(interpolated_all, ignore_index=True)\n",
        "\n",
        "# 6. Simpan ke Excel / CSV\n",
        "df_final.to_csv('/content/drive/MyDrive/Interpolated_GSMaP_Indonesia.csv', index=False)\n",
        "print(\"Interpolasi selesai dan disimpan ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avV5hXTx7DVT",
        "outputId": "25665658-39c0-40b3-cfdb-8ff2f4726ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolasi selesai dan disimpan ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ERA5"
      ],
      "metadata": {
        "id": "nanNsSdpuRYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_points = pd.read_excel('/content/drive/MyDrive/Shared/Koordinat CH.xlsx')\n",
        "\n",
        "ds = xr.open_mfdataset('/content/drive/MyDrive/Shared/00 Hujan/ERA5/*.nc', combine=\"by_coords\")\n",
        "\n",
        "# === 1. Baca koordinat titik dari Excel ===\n",
        "lat_points = df_points[\"Y\"].values\n",
        "lon_points = df_points[\"X\"].values\n",
        "point_coords = np.array([lat_points, lon_points]).T\n",
        "\n",
        "# === 2. Baca data NetCDF ===\n",
        "tp = ds[\"tp\"]  # (valid_time, lat, lon)\n",
        "lats = ds[\"latitude\"].values\n",
        "lons = ds[\"longitude\"].values\n",
        "times = pd.to_datetime(ds[\"valid_time\"].values)\n",
        "\n",
        "# Tambahkan koordinat waktu datetime\n",
        "tp = tp.assign_coords(valid_time=times)\n",
        "\n",
        "# Tambahkan koordinat tahun dan bulan untuk groupby\n",
        "tp.coords[\"year\"] = (\"valid_time\", tp[\"valid_time\"].dt.year.data)\n",
        "tp.coords[\"month\"] = (\"valid_time\", tp[\"valid_time\"].dt.month.data)\n",
        "\n",
        "# === 3. Agregasi bulanan di grid ===\n",
        "tp_monthly = tp.groupby([\"year\", \"month\"]).sum(dim=\"valid_time\")  # hasil: (year, month, lat, lon)\n",
        "\n",
        "# === 4. Interpolasi ke titik Excel per bulan ===\n",
        "results = []\n",
        "\n",
        "for i in range(tp_monthly[\"year\"].size):\n",
        "    for j in range(tp_monthly[\"month\"].size):\n",
        "        try:\n",
        "            tp_sel = tp_monthly.isel(year=i, month=j)\n",
        "            year_val = int(tp_sel[\"year\"].values)\n",
        "            month_val = int(tp_sel[\"month\"].values)\n",
        "            tp_vals = tp_sel.values  # (lat, lon)\n",
        "\n",
        "            # Interpolator bilinear\n",
        "            interpolator = RegularGridInterpolator((lats, lons), tp_vals, bounds_error=False, fill_value=np.nan)\n",
        "            interpolated = interpolator(point_coords)\n",
        "\n",
        "            # Nearest Neighbor fallback\n",
        "            nan_mask = np.isnan(interpolated)\n",
        "            if nan_mask.any():\n",
        "                valid_mask = ~np.isnan(tp_vals)\n",
        "                lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')\n",
        "                valid_points = np.array([lat_grid[valid_mask], lon_grid[valid_mask]]).T\n",
        "                valid_values = tp_vals[valid_mask]\n",
        "                nearest_idx = np.argmin(cdist(point_coords[nan_mask], valid_points), axis=1)\n",
        "                interpolated[nan_mask] = valid_values[nearest_idx]\n",
        "\n",
        "            # Simpan ke DataFrame\n",
        "            df_out = df_points.copy()\n",
        "            df_out[\"tp_interpolated\"] = interpolated * 1000 # dari m ke mm\n",
        "            df_out[\"tahun\"] = year_val\n",
        "            df_out[\"bulan\"] = month_val\n",
        "            results.append(df_out)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skip year={i}, month={j} karena error: {e}\")\n",
        "            continue\n",
        "\n",
        "# Gabung semua\n",
        "df_all = pd.concat(results, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "hZmczUd8uTLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tEv682DfGOEL",
        "cA4_z_1XB-gc",
        "NLvrR5rfREr9",
        "2x17HNCNBy0s"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}